{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988d8c35-d2b5-43ad-a232-5bf4e2d6450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on April , 2021\n",
    "@author:\n",
    "'''\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import RecurrencePlot\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.decomposition import PCA\n",
    "# from pyts.approximation import SymbolicFourierApproximation\n",
    "\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "\n",
    "\n",
    "class input_gen(object):\n",
    "    '''\n",
    "    class for data preparation (rps generator)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data_path_list, sequence_length, sensor_drop, piecewise_lin_ref=125, preproc=False, visualize=False):\n",
    "        '''\n",
    "        :param data_path_list: python list of four sub-dataset\n",
    "        :param sequence_length: legnth of sequence (sliced time series)\n",
    "        :param sensor_drop: sensors not to be considered\n",
    "        :param piecewise_lin_ref: max rul value (if real rul value is larger than piecewise_lin_ref,\n",
    "        then the rul value is piecewise_lin_ref)\n",
    "        :param preproc: preprocessing\n",
    "        '''\n",
    "        # self.__logger = logging.getLogger('data preparation for using it as the network input')\n",
    "        self.data_path_list = data_path_list\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sensor_drop = sensor_drop\n",
    "        self.preproc = preproc\n",
    "        self.piecewise_lin_ref = piecewise_lin_ref\n",
    "        self.visualize = visualize\n",
    "\n",
    "\n",
    "        ## Assign columns name\n",
    "        cols = ['unit_nr', 'cycles', 'os_1', 'os_2', 'os_3']\n",
    "        cols += ['sensor_{0:02d}'.format(s + 1) for s in range(26)]\n",
    "        col_rul = ['RUL_truth']\n",
    "\n",
    "        train_FD = pd.read_csv(self.data_path_list[0], sep=' ', header=None,\n",
    "                               names=cols, index_col=False)\n",
    "        test_FD = pd.read_csv(self.data_path_list[1], sep=' ', header=None,\n",
    "                              names=cols, index_col=False)\n",
    "        RUL_FD = pd.read_csv(self.data_path_list[2], sep=' ', header=None,\n",
    "                             names=col_rul, index_col=False)\n",
    "\n",
    "        ## Calculate RUL and append to train data\n",
    "        # get the time of the last available measurement for each unit\n",
    "        mapper = {}\n",
    "        for unit_nr in train_FD['unit_nr'].unique():\n",
    "            mapper[unit_nr] = train_FD['cycles'].loc[train_FD['unit_nr'] == unit_nr].max()\n",
    "\n",
    "        # calculate RUL = time.max() - time_now for each unit\n",
    "        train_FD['RUL'] = train_FD['unit_nr'].apply(lambda nr: mapper[nr]) - train_FD['cycles']\n",
    "        # piecewise linear for RUL labels\n",
    "        train_FD['RUL'].loc[(train_FD['RUL'] > self.piecewise_lin_ref)] = self.piecewise_lin_ref\n",
    "\n",
    "        # Cut max RUL ground truth\n",
    "        RUL_FD['RUL_truth'].loc[(RUL_FD['RUL_truth'] > self.piecewise_lin_ref)] = self.piecewise_lin_ref\n",
    "\n",
    "        ## Excluse columns which only have NaN as value\n",
    "        # nan_cols = ['sensor_{0:02d}'.format(s + 22) for s in range(5)]\n",
    "        cols_nan = train_FD.columns[train_FD.isna().any()].tolist()\n",
    "        # print('Columns with all nan: \\n' + str(cols_nan) + '\\n')\n",
    "        cols_const = [col for col in train_FD.columns if len(train_FD[col].unique()) <= 2]\n",
    "        # print('Columns with all const values: \\n' + str(cols_const) + '\\n')\n",
    "\n",
    "        ## Drop exclusive columns\n",
    "        # train_FD = train_FD.drop(columns=cols_const + cols_nan)\n",
    "        # test_FD = test_FD.drop(columns=cols_const + cols_nan)\n",
    "\n",
    "        train_FD = train_FD.drop(columns=cols_const + cols_nan + sensor_drop)\n",
    "\n",
    "        test_FD = test_FD.drop(columns=cols_const + cols_nan + sensor_drop)\n",
    "\n",
    "\n",
    "        if self.preproc == True:\n",
    "            ## preprocessing(normailization for the neural networks)\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            # for the training set\n",
    "            # train_FD['cycles_norm'] = train_FD['cycles']\n",
    "            cols_normalize = train_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2', 'RUL'])\n",
    "\n",
    "            norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_FD[cols_normalize]),\n",
    "                                         columns=cols_normalize,\n",
    "                                         index=train_FD.index)\n",
    "            join_df = train_FD[train_FD.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "            train_FD = join_df.reindex(columns=train_FD.columns)\n",
    "\n",
    "            # for the test set\n",
    "            # test_FD['cycles_norm'] = test_FD['cycles']\n",
    "            cols_normalize_test = test_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2'])\n",
    "            # print (\"cols_normalize_test\", cols_normalize_test)\n",
    "            norm_test_df = pd.DataFrame(min_max_scaler.transform(test_FD[cols_normalize_test]), columns=cols_normalize_test,\n",
    "                                        index=test_FD.index)\n",
    "            test_join_df = test_FD[test_FD.columns.difference(cols_normalize_test)].join(norm_test_df)\n",
    "            test_FD = test_join_df.reindex(columns=test_FD.columns)\n",
    "            test_FD = test_FD.reset_index(drop=True)\n",
    "        else:\n",
    "            # print (\"No preprocessing\")\n",
    "            pass\n",
    "\n",
    "        # Specify the columns to be used\n",
    "        sequence_cols_train = train_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2', 'RUL'])\n",
    "        sequence_cols_test = test_FD.columns.difference(['unit_nr', 'os_1', 'os_2', 'cycles'])\n",
    "\n",
    "\n",
    "\n",
    "        ## generator for the sequences\n",
    "        # transform each id of the train dataset in a sequence\n",
    "        seq_gen = (list(gen_sequence(train_FD[train_FD['unit_nr'] == id], self.sequence_length, sequence_cols_train))\n",
    "                   for id in train_FD['unit_nr'].unique())\n",
    "\n",
    "        # generate sequences and convert to numpy array in training set\n",
    "        seq_array_train = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "        self.seq_array_train = seq_array_train.transpose(0, 2, 1) # shape = (samples, sensors, sequences)\n",
    "        # print(\"seq_array_train.shape\", self.seq_array_train.shape)\n",
    "\n",
    "        # generate label of training samples\n",
    "        label_gen = [gen_labels(train_FD[train_FD['unit_nr'] == id], self.sequence_length, ['RUL'])\n",
    "                     for id in train_FD['unit_nr'].unique()]\n",
    "        self.label_array_train = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "        # generate sequences and convert to numpy array in test set (only the last sequence for each engine in test set)\n",
    "        seq_array_test_last = [test_FD[test_FD['unit_nr'] == id][sequence_cols_test].values[-self.sequence_length:]\n",
    "                               for id in test_FD['unit_nr'].unique() if\n",
    "                               len(test_FD[test_FD['unit_nr'] == id]) >= self.sequence_length]\n",
    "\n",
    "        seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "        self.seq_array_test_last = seq_array_test_last.transpose(0, 2, 1) # shape = (samples, sensors, sequences)\n",
    "        # print(\"seq_array_test_last.shape\", self.seq_array_test_last.shape)\n",
    "\n",
    "        # generate label of test samples\n",
    "        y_mask = [len(test_FD[test_FD['unit_nr'] == id]) >= self.sequence_length for id in test_FD['unit_nr'].unique()]\n",
    "        label_array_test_last = RUL_FD['RUL_truth'][y_mask].values\n",
    "        self.label_array_test = label_array_test_last.reshape(label_array_test_last.shape[0], 1).astype(np.float32)\n",
    "\n",
    "\n",
    "        ## Visualize Run-2-failure TS of the first engine in the training set.(Please deactivate after understanding)\n",
    "        if self.visualize == True:\n",
    "            # R2F TS of the first engine\n",
    "            pd.DataFrame(train_FD[train_FD['unit_nr'] == 1][sequence_cols_train].values,\n",
    "                             columns=sequence_cols_train).plot(subplots=True, figsize=(15, 15))\n",
    "\n",
    "            # The last sequences sliced from each TS (of the first engine)\n",
    "            prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "            colors = prop_cycle.by_key()['color']\n",
    "            colors = colors + colors + colors\n",
    "\n",
    "            seq_gen = (\n",
    "            list(gen_sequence(train_FD[train_FD['unit_nr'] == id], self.sequence_length, sequence_cols_train))\n",
    "            for id in train_FD['unit_nr'].unique())\n",
    "\n",
    "            seq_list_engine = list(seq_gen)\n",
    "            seq_engine_1_array = np.asarray(seq_list_engine[0])\n",
    "\n",
    "            last_seq_engine_1_array = seq_engine_1_array[-1, :, :]\n",
    "            fig_ts = plt.figure(figsize=(15, 15))\n",
    "            for s in range(last_seq_engine_1_array.shape[1]):\n",
    "                seq_s = last_seq_engine_1_array[:, s]\n",
    "                # plt.subplot(last_seq_engine_1_array.shape[1],(s//4) + 1, (s%4)+1)\n",
    "                plt.subplot(4, 4, s + 1)\n",
    "                plt.plot(seq_s, \"y\", label=sequence_cols_train[s], color=colors[s])\n",
    "                plt.legend()\n",
    "\n",
    "            plt.xlabel(\"time(cycles)\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def rps(self, thres_type=None, thres_percentage=50, flatten=False, visualize=True):\n",
    "        '''\n",
    "        generate RPs from sequences\n",
    "        :param thres_type:  ‘point’, ‘distance’ or None (default = None)\n",
    "        :param thres_percentage:\n",
    "        :param flatten:\n",
    "        :param visualize: visualize generated RPs (first training sample)\n",
    "        :return: PRs (samples for NNs and their label)\n",
    "        '''\n",
    "\n",
    "        # Recurrence plot transformation for training samples\n",
    "        rp_train = RecurrencePlot(threshold=thres_type, percentage=thres_percentage,\n",
    "                                  flatten=flatten)\n",
    "\n",
    "        rp_list = []\n",
    "        for idx in range(self.seq_array_train.shape[0]):\n",
    "            temp_mts = self.seq_array_train[idx]\n",
    "            # print (temp_mts.shape)\n",
    "            X_rp_temp = rp_train.fit_transform(temp_mts)\n",
    "            # print (X_rp_temp.shape)\n",
    "            rp_list.append(X_rp_temp)\n",
    "\n",
    "        rp_train_samples = np.stack(rp_list, axis=0)\n",
    "\n",
    "        # Recurrence plot transformation for test samples\n",
    "        rp_test = RecurrencePlot(threshold=thres_type, percentage=thres_percentage, flatten=flatten)\n",
    "        rp_list = []\n",
    "        for idx in range(self.seq_array_test_last.shape[0]):\n",
    "            temp_mts = self.seq_array_test_last[idx]\n",
    "            # print (temp_mts.shape)\n",
    "            X_rp_temp = rp_test.fit_transform(temp_mts)\n",
    "            # print (X_rp_temp.shape)\n",
    "            rp_list.append(X_rp_temp)\n",
    "        rp_test_samples = np.stack(rp_list, axis=0)\n",
    "\n",
    "        label_array_train = self.label_array_train\n",
    "        label_array_test = self.label_array_test\n",
    "\n",
    "        # Visualize RPs of the last sequences sliced from each TS (of the first engine)\n",
    "        if visualize == True:\n",
    "            X_rp = rp_train_samples[-1]\n",
    "            plt.figure(figsize=(15, 15))\n",
    "            for s in range(len(X_rp)):\n",
    "                # plt.subplot(last_seq_engine_1_array.shape[1],(s//4) + 1, (s%4)+1)\n",
    "                plt.subplot(4, 4, s + 1)\n",
    "                if flatten == True:\n",
    "                    img = np.atleast_2d(X_rp[s])\n",
    "                    plt.imshow(img, extent=(0, img.shape[1], 0, round(img.shape[1]/9)))\n",
    "                else:\n",
    "                    plt.imshow(X_rp[s], origin='lower')\n",
    "                # plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        return  rp_train_samples, label_array_train, rp_test_samples, label_array_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a66cbd1-a1a6-4194-9f28-941bc25e9466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import logging as log\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import importlib\n",
    "from scipy.stats import randint, expon, uniform\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from math import sqrt\n",
    "# import keras\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# import keras.backend as K\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LSTM, TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "def gen_net(vec_len, num_hidden1, num_hidden2 ):\n",
    "    '''\n",
    "    TODO: Generate and evaluate any CNN instead of MLPs\n",
    "    :param vec_len:\n",
    "    :param num_hidden1:\n",
    "    :param num_hidden2:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_hidden1, activation='relu', input_shape=(vec_len,)))\n",
    "    model.add(Dense(num_hidden2, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class network_fit(object):\n",
    "    '''\n",
    "    class for network\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_samples, label_array_train, test_samples, label_array_test,\n",
    "                 model_path, n_hidden1 =100, n_hidden2 =10, verbose=1):\n",
    "        '''\n",
    "        Constructor\n",
    "        Generate a NN and train\n",
    "        @param none\n",
    "        '''\n",
    "        # self.__logger = logging.getLogger('data preparation for using it as the network input')\n",
    "        self.train_samples = train_samples\n",
    "        self.label_array_train = label_array_train\n",
    "        self.test_samples = test_samples\n",
    "        self.label_array_test = label_array_test\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.model_path = model_path\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.mlps = gen_net(self.train_samples.shape[1], self.n_hidden1, self.n_hidden2)\n",
    "\n",
    "\n",
    "\n",
    "    def train_net(self, epochs = 1000, batch_size= 700, lr= 1e-05, plotting=True):\n",
    "        '''\n",
    "        specify the optimizers and train the network\n",
    "        :param epochs:\n",
    "        :param batch_size:\n",
    "        :param lr:\n",
    "        :return:\n",
    "        '''\n",
    "        print(\"Initializing network...\")\n",
    "        # compile the model\n",
    "        rp = optimizers.RMSprop(learning_rate=lr, rho=0.9, centered=True)\n",
    "        adm = optimizers.Adam(learning_rate=lr, epsilon=1)\n",
    "        sgd_m = optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "        keras_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "        self.mlps.compile(loss='mean_squared_error', optimizer=sgd_m, metrics=[keras_rmse, 'mae'])\n",
    "\n",
    "        # print(self.mlps.summary())\n",
    "\n",
    "        # Train the model\n",
    "        history = self.mlps.fit(self.train_samples, self.label_array_train, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_split=0.2, verbose=self.verbose,\n",
    "                                callbacks=[\n",
    "                               EarlyStopping(monitor='val_root_mean_squared_error', min_delta=0, patience=50, verbose=self.verbose, mode='min'),\n",
    "                               ModelCheckpoint(self.model_path, monitor='val_root_mean_squared_error', save_best_only=True, mode='min',\n",
    "                                               verbose=self.verbose)])\n",
    "\n",
    "        val_rmse_k = history.history['val_root_mean_squared_error']\n",
    "        val_rmse_min = min(val_rmse_k)\n",
    "        min_val_rmse_idx = val_rmse_k.index(min(val_rmse_k))\n",
    "        stop_epoch = min_val_rmse_idx +1\n",
    "        val_rmse_min = round(val_rmse_min, 4)\n",
    "        print (\"val_rmse_min: \", val_rmse_min)\n",
    "\n",
    "        trained_net = self.mlps\n",
    "\n",
    "        ## Plot training & validation loss about epochs\n",
    "        if plotting == True:\n",
    "            # summarize history for Loss\n",
    "            fig_acc = plt.figure(figsize=(10, 10))\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.ylim(0, 2000)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        return trained_net\n",
    "\n",
    "\n",
    "\n",
    "    def test_net(self, trained_net=None, best_model=True, plotting=True):\n",
    "        '''\n",
    "        Evalute the trained network on test set\n",
    "        :param trained_net:\n",
    "        :param best_model:\n",
    "        :param plotting:\n",
    "        :return:\n",
    "        '''\n",
    "        # Load the trained model\n",
    "        if best_model:\n",
    "            estimator = load_model(self.model_path)\n",
    "        else:\n",
    "            estimator = load_model(trained_net)\n",
    "\n",
    "        # predict the RUL\n",
    "        y_pred_test = estimator.predict(self.test_samples)\n",
    "        y_true_test = self.label_array_test # ground truth of test samples\n",
    "\n",
    "        pd.set_option('display.max_rows', 1000)\n",
    "        test_print = pd.DataFrame()\n",
    "        test_print['y_pred'] = y_pred_test.flatten()\n",
    "        test_print['y_truth'] = y_true_test.flatten()\n",
    "        test_print['diff'] = abs(y_pred_test.flatten() - y_true_test.flatten())\n",
    "        test_print['diff(ratio)'] = abs(y_pred_test.flatten() - y_true_test.flatten()) / y_true_test.flatten()\n",
    "        test_print['diff(%)'] = (abs(y_pred_test.flatten() - y_true_test.flatten()) / y_true_test.flatten()) * 100\n",
    "\n",
    "        y_predicted = test_print['y_pred']\n",
    "        y_actual = test_print['y_truth']\n",
    "        rms = sqrt(mean_squared_error(y_actual, y_predicted)) # RMSE metric\n",
    "        test_print['rmse'] = rms\n",
    "        print(test_print)\n",
    "\n",
    "\n",
    "        # Score metric\n",
    "        h_array = y_predicted - y_actual\n",
    "        s_array = np.zeros(len(h_array))\n",
    "        for j, h_j in enumerate(h_array):\n",
    "            if h_j < 0:\n",
    "                s_array[j] = math.exp(-(h_j / 13)) - 1\n",
    "\n",
    "            else:\n",
    "                s_array[j] = math.exp(h_j / 10) - 1\n",
    "        score = np.sum(s_array)\n",
    "\n",
    "        # Plot the results of RUL prediction\n",
    "        if plotting == True:\n",
    "            fig_verify = plt.figure(figsize=(12, 6))\n",
    "            plt.plot(y_pred_test, color=\"blue\")\n",
    "            plt.plot(y_true_test, color=\"green\")\n",
    "            plt.title('prediction')\n",
    "            plt.ylabel('value')\n",
    "            plt.xlabel('row')\n",
    "            plt.legend(['predicted', 'actual data'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "        return rms, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637ee5a6-907c-42eb-9549-9fabed4c2cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1\\ipykernel_13412\\3991885269.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mcurrent_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m## Dataset path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "## Import libraries in python\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import importlib\n",
    "from scipy.stats import randint, expon, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Ignore tf err log\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# random seed predictable\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "## Dataset path\n",
    "train_FD001_path = current_dir +'/cmapss/train_FD001.csv'\n",
    "test_FD001_path = current_dir +'/cmapss/test_FD001.csv'\n",
    "RUL_FD001_path = current_dir+'/cmapss/RUL_FD001.txt'\n",
    "FD001_path = [train_FD001_path, test_FD001_path, RUL_FD001_path]\n",
    "\n",
    "train_FD002_path = current_dir +'/cmapss/train_FD002.csv'\n",
    "test_FD002_path = current_dir +'/cmapss/test_FD002.csv'\n",
    "RUL_FD002_path = current_dir +'/cmapss/RUL_FD002.txt'\n",
    "FD002_path = [train_FD002_path, test_FD002_path, RUL_FD002_path]\n",
    "\n",
    "train_FD003_path = current_dir +'/cmapss/train_FD003.csv'\n",
    "test_FD003_path = current_dir +'/cmapss/test_FD003.csv'\n",
    "RUL_FD003_path = current_dir +'/cmapss/RUL_FD003.txt'\n",
    "FD003_path = [train_FD003_path, test_FD003_path, RUL_FD003_path]\n",
    "\n",
    "train_FD004_path =current_dir +'/cmapss/train_FD004.csv'\n",
    "test_FD004_path = current_dir +'/cmapss/test_FD004.csv'\n",
    "RUL_FD004_path = current_dir +'/cmapss/RUL_FD004.txt'\n",
    "FD004_path = [train_FD004_path, test_FD004_path, RUL_FD004_path]\n",
    "\n",
    "## Assign columns name\n",
    "cols = ['unit_nr', 'cycles', 'os_1', 'os_2', 'os_3']\n",
    "cols += ['sensor_{0:02d}'.format(s + 1) for s in range(26)]\n",
    "col_rul = ['RUL_truth']\n",
    "\n",
    "## Read csv file to pandas dataframe\n",
    "FD_path = [\"none\", FD001_path, FD002_path, FD003_path, FD004_path]\n",
    "dp_str = [\"none\", \"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
    "\n",
    "## temporary model path for NN\n",
    "model_path = current_dir +'/temp_net.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc85b7-bd0a-45f1-8c87-5a93bb25d02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
