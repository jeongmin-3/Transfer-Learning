{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e0c774-4bf7-40b8-afa8-ef1ae338b55d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at C:\\Users\\Administrator\\RUL2\\cmapss/temp_net.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1\\ipykernel_22468\\1160715566.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;31m# Load the trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;31m# Predict RUL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\RUL\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\RUL\\lib\\site-packages\\keras\\saving\\legacy\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         raise IOError(\n\u001b[1;32m--> 228\u001b[1;33m                             \u001b[1;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                         )\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at C:\\Users\\Administrator\\RUL2\\cmapss/temp_net.h5"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pyts.image import RecurrencePlot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    \n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# Define paths\n",
    "current_dir = os.getcwd()\n",
    "train_FD001_path = current_dir +'/cmapss/train_FD001.csv'\n",
    "test_FD001_path = current_dir +'/cmapss/test_FD001.csv'\n",
    "RUL_FD001_path = current_dir+'/cmapss/RUL_FD001.txt'\n",
    "model_path = current_dir + '/temp_net.h5'\n",
    "\n",
    "# Load train and test data\n",
    "train_FD = pd.read_csv(train_FD001_path, sep=' ', header=None).drop([26, 27], axis=1)\n",
    "train_FD.columns = ['unit_nr', 'cycles', 'os_1', 'os_2', 'os_3'] + ['sensor_{0:02d}'.format(s + 1) for s in range(21)]\n",
    "test_FD = pd.read_csv(test_FD001_path, sep=' ', header=None).drop([26, 27], axis=1)\n",
    "test_FD.columns = ['unit_nr', 'cycles', 'os_1', 'os_2', 'os_3'] + ['sensor_{0:02d}'.format(s + 1) for s in range(21)]\n",
    "RUL_FD = pd.read_csv(RUL_FD001_path, sep=' ', header=None)\n",
    "RUL_FD = RUL_FD.dropna(axis=1, how='all')\n",
    "RUL_FD.columns = ['RUL_truth']\n",
    "\n",
    "# Calculate RUL and append to train data\n",
    "mapper = {}\n",
    "for unit_nr in train_FD['unit_nr'].unique():\n",
    "    mapper[unit_nr] = train_FD['cycles'].loc[train_FD['unit_nr'] == unit_nr].max()\n",
    "\n",
    "train_FD['RUL'] = train_FD['unit_nr'].apply(lambda nr: mapper[nr]) - train_FD['cycles']\n",
    "piecewise_lin_ref = 125\n",
    "train_FD['RUL'] = train_FD['RUL'].apply(lambda x: x if x <= piecewise_lin_ref else piecewise_lin_ref)\n",
    "RUL_FD['RUL_truth'] = RUL_FD['RUL_truth'].apply(lambda x: x if x <= piecewise_lin_ref else piecewise_lin_ref)\n",
    "\n",
    "# Min-Max normalization\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# For the training set\n",
    "cols_normalize = train_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2', 'RUL'])\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_FD[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_FD.index)\n",
    "join_df = train_FD[train_FD.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_FD = join_df.reindex(columns=train_FD.columns)\n",
    "\n",
    "# For the test set\n",
    "cols_normalize_test = test_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2'])\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_FD[cols_normalize_test]), \n",
    "                            columns=cols_normalize_test, \n",
    "                            index=test_FD.index)\n",
    "test_join_df = test_FD[test_FD.columns.difference(cols_normalize_test)].join(norm_test_df)\n",
    "test_FD = test_join_df.reindex(columns=test_FD.columns)\n",
    "test_FD = test_FD.reset_index(drop=True)\n",
    "\n",
    "# Exclude columns which only have NaN as value or constant values\n",
    "cols_nan = train_FD.columns[train_FD.isna().any()].tolist()\n",
    "cols_const = [col for col in train_FD.columns if len(train_FD[col].unique()) <= 2]\n",
    "sensor_drop = ['sensor_01', 'sensor_05', 'sensor_06', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
    "\n",
    "train_FD = train_FD.drop(columns=cols_const + cols_nan + sensor_drop)\n",
    "test_FD = test_FD.drop(columns=cols_const + cols_nan + sensor_drop)\n",
    "\n",
    "# Specify the columns to be used\n",
    "sequence_cols_train = train_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2', 'RUL'])\n",
    "sequence_cols_test = test_FD.columns.difference(['unit_nr', 'os_1', 'os_2', 'cycles'])\n",
    "\n",
    "# Generator for the sequences\n",
    "sequence_length = 30\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. \"\"\"\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]\n",
    "\n",
    "# Generate sequences for the training set\n",
    "seq_gen = (list(gen_sequence(train_FD[train_FD['unit_nr'] == id], sequence_length, sequence_cols_train))\n",
    "           for id in train_FD['unit_nr'].unique())\n",
    "seq_array_train = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array_train = seq_array_train.transpose(0, 2, 1)\n",
    "\n",
    "label_gen = [gen_labels(train_FD[train_FD['unit_nr'] == id], sequence_length, ['RUL'])\n",
    "             for id in train_FD['unit_nr'].unique()]\n",
    "label_array_train = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "# Generate sequences for the test set (only the last sequence for each engine in test set)\n",
    "seq_array_test_last = [test_FD[test_FD['unit_nr'] == id][sequence_cols_test].values[-sequence_length:]\n",
    "                       for id in test_FD['unit_nr'].unique() if\n",
    "                       len(test_FD[test_FD['unit_nr'] == id]) >= sequence_length]\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "seq_array_test_last = seq_array_test_last.transpose(0, 2, 1)\n",
    "\n",
    "# Recurrence plot transformation for training samples\n",
    "thres_type = None\n",
    "thres_percentage = 50\n",
    "flatten = False\n",
    "\n",
    "rp_train = RecurrencePlot(threshold=thres_type, percentage=thres_percentage, flatten=flatten)\n",
    "rp_list_train = []\n",
    "for idx in range(seq_array_train.shape[0]):\n",
    "    temp_mts = seq_array_train[idx]\n",
    "    X_rp_temp = rp_train.fit_transform(temp_mts)\n",
    "    rp_list_train.append(X_rp_temp)\n",
    "rp_train_samples = np.stack(rp_list_train, axis=0)\n",
    "\n",
    "# Recurrence plot transformation for test samples\n",
    "rp_test = RecurrencePlot(threshold=thres_type, percentage=thres_percentage, flatten=flatten)\n",
    "rp_list_test = []\n",
    "for idx in range(seq_array_test_last.shape[0]):\n",
    "    temp_mts = seq_array_test_last[idx]\n",
    "    X_rp_temp = rp_test.fit_transform(temp_mts)\n",
    "    rp_list_test.append(X_rp_temp)\n",
    "rp_test_samples = np.stack(rp_list_test, axis=0)\n",
    "\n",
    "# Reshape data to fit the model (add channel dimension)\n",
    "rp_train_samples = rp_train_samples.reshape(rp_train_samples.shape[0], rp_train_samples.shape[1], rp_train_samples.shape[2], rp_train_samples.shape[3], 1)\n",
    "rp_test_samples = rp_test_samples.reshape(rp_test_samples.shape[0], rp_test_samples.shape[1], rp_test_samples.shape[2], rp_test_samples.shape[3], 1)\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# Predict RUL\n",
    "y_pred_test = model.predict(rp_test_samples).flatten()\n",
    "\n",
    "# Load true RUL values and mask to get only engines with sufficient sequence length\n",
    "y_mask = [len(test_FD[test_FD['unit_nr'] == id]) >= sequence_length for id in test_FD['unit_nr'].unique()]\n",
    "true_rul = RUL_FD['RUL_truth'][y_mask].values\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(true_rul, y_pred_test))\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "\n",
    "# Plot RUL truth vs. predicted RUL for each engine\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(true_rul, marker='o', linestyle='--', color='b', label='True RUL')\n",
    "plt.plot(y_pred_test, marker='x', linestyle='-', color='r', label='Predicted RUL')\n",
    "plt.title('True RUL vs. Predicted RUL for Each Engine')\n",
    "plt.xlabel('Engine Index')\n",
    "plt.ylabel('RUL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58e107-9e7b-4710-bc0e-f654b5e6557a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
